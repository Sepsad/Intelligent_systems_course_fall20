{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<body >\n",
    "    <div style=\"direction:rtl;line-height:300%;background-color:rgb(255, 255, 255);\">\n",
    "            <img src=\"./images/Fanni-Transparent.png\" style=\"position:reletive;margin-top:25pt;float:right;\" width=\"200\" height=\"200\"/>\n",
    "    <img src=\"./images/University_of_Tehran_logo.svg\" style=\"position:reletive;margin-top:25pt;margin-left:20pt;float:left;\" width=\"150\" height=\"150\"/>\n",
    "\t\t<div align=center>\n",
    "\t\t\t<font  size=5 >\n",
    "\t\t\t\t<p></p>\n",
    "\t\t\t\t<p></p>\n",
    "                <br>\n",
    "                 بسمه تعالی\n",
    "\t\t\t\t<p></p>\n",
    "\t\t\t</font>\n",
    "\t\t</div>\n",
    "        <div align=center >\n",
    "                        <font size=30 >\n",
    "                            <br>\n",
    "                            گزارش همورک چهارم\n",
    "                        </font>\n",
    "         </div>\n",
    "           <div align=center >\n",
    "                        <font size=3 >\n",
    "                            <br>\n",
    "تهیه و تنظیم : سپهر سعیدپور\n",
    "                            <br>\n",
    "۸۱۰۱۹۶۶۵۴\n",
    "                        </font>\n",
    "        </div>\n",
    "    </div>\n",
    "</body>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# پیاده‌سازی K means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Kmeans(object):\n",
    "    def __init__(self, data, num_clusters):\n",
    "        \n",
    "        self.k = num_clusters\n",
    "        self.data = data\n",
    "    \n",
    "    def get_initial_centroids(self,seed=None):\n",
    "        if seed != None:  \n",
    "            np.random.seed(seed)\n",
    "        \n",
    "        n = self.data.shape[0]  \n",
    "        rand_indices = (np.random.choice(range(n), self.k , replace = False))\n",
    "\n",
    "        centroids = self.data.iloc[rand_indices, :]\n",
    "\n",
    "        return np.array(centroids)\n",
    "    \n",
    "    \n",
    "    def get_euclidean_distance(self,B):\n",
    "        A = np.array(self.data)\n",
    "        return np.array([[ np.linalg.norm(i-j) for j in B] for i in A])\n",
    "    \n",
    "    \n",
    "    def get_clusters(self, centroids):\n",
    "        \n",
    "        clusters = {}\n",
    "        distance_matrix = self.get_euclidean_distance(centroids)\n",
    "        closest_cluster_ids = np.argmin(distance_matrix, axis=1)\n",
    "\n",
    "        for i in range(self.k):\n",
    "            clusters[i] = []\n",
    "\n",
    "        for i, cluster_id in enumerate(closest_cluster_ids):\n",
    "            clusters[cluster_id].append(self.data.iloc[i])\n",
    "    \n",
    "        return clusters , closest_cluster_ids\n",
    "    \n",
    "    def has_centroids_covered(self,previous_centroids, centroids, threshold):\n",
    "        \n",
    "        distances_between_old_and_new_centroids = get_euclidean_distance(previous_centroids, centroids)\n",
    "        centroids_covered = (np.max(distances_between_old_and_new_centroids.diagonal()) <= threshold)\n",
    "\n",
    "        return centroids_covered\n",
    "    \n",
    "    \n",
    "    \n",
    "    def train(self,threshold):\n",
    "        \n",
    "        centroids = self.get_initial_centroids(10)\n",
    "\n",
    "        centroids_covered = False\n",
    "\n",
    "        while not centroids_covered:\n",
    "            previous_centroids = centroids\n",
    "            clusters, ids = self.get_clusters(previous_centroids)\n",
    "\n",
    "            centroids = np.array([np.mean(clusters[key], axis=0) for key in sorted(clusters.keys())])\n",
    "\n",
    "            centroids_covered = self.has_centroids_covered(previous_centroids, centroids, threshold)\n",
    "\n",
    "        return centroids, ids\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv('Q1/Train_Data.csv', header= None)\n",
    "train_labels = pd.read_csv('Q1/Train_Labels.csv', header= None)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = Kmeans(train_data, num_clusters= 3)\n",
    "\n",
    "labels = model.train(0)[1]\n",
    "\n",
    "labels = pd.Series(labels).replace({1 : 0 , 2: 1, 0 : 2})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # پیاده‌سازی k means به وسیله کتابخانه "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "km = KMeans(\n",
    "    n_clusters=3, init='random',\n",
    "    n_init=10, max_iter=300, \n",
    "    tol=1e-04, random_state=10\n",
    ")\n",
    "y_km = km.fit_predict(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 0, 0, 2, 2, 1, 2, 0, 2, 1, 1, 0, 1, 2, 2, 0, 2, 1, 1, 0, 2,\n",
       "       2, 1, 2, 2, 1, 2, 0, 0, 0, 1, 0, 1, 2, 2, 0, 0, 1, 0, 1, 2, 0, 2,\n",
       "       1, 2, 2, 2, 0, 2, 0, 2, 2, 1, 2, 2, 2, 1, 2, 0, 0, 0, 1, 2, 0, 0,\n",
       "       0, 2, 1, 1, 2, 0, 2, 0, 0, 2, 2, 1, 2, 2, 2, 1, 2, 0, 0, 0, 2, 0,\n",
       "       0, 1, 0, 2, 0, 2, 2, 2, 0, 1, 1, 0, 1, 2, 2, 2, 1, 0, 1, 1, 2, 2,\n",
       "       1, 2, 2, 0, 2, 2, 2, 2, 1, 0, 0, 0, 2, 2, 1, 2, 2, 0, 1, 2, 2, 0,\n",
       "       1, 0, 2, 0, 2, 2, 2, 0, 2, 2, 0, 0, 1])"
      ]
     },
     "execution_count": 277,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_km"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  محاسبه Confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[47,  0,  0],\n",
       "       [ 0, 34,  0],\n",
       "       [ 1,  0, 63]], dtype=int64)"
      ]
     },
     "execution_count": 279,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "confusion_matrix(y_km, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./2.jpg\" width=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.        , 15.16575089, 11.22497216, 14.35270009,  5.47722558],\n",
       "       [15.16575089,  0.        , 17.72004515,  7.34846923, 13.92838828],\n",
       "       [11.22497216, 17.72004515,  0.        , 12.08304597,  9.79795897],\n",
       "       [14.35270009,  7.34846923, 12.08304597,  0.        , 11.74734012],\n",
       "       [ 5.47722558, 13.92838828,  9.79795897, 11.74734012,  0.        ]])"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.spatial import distance_matrix\n",
    "\n",
    "data = [[12,9,7], [-2, 4, 4], [15, 0, 1], [3, -1, 2], [11, 4, 9]]\n",
    "\n",
    "distance_matrix(data,data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./3.jpg\" width=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "شماره سوال به اشتباه درج شده‌است"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Q4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 659,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy\n",
    "from scipy.stats import multivariate_normal\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import preprocessing\n",
    "min_max_scaler = preprocessing.StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 660,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv('Q4/Train_Data.csv', header= None)\n",
    "train_labels = pd.read_csv('Q4/Train_Labels.csv', header= None)[0]\n",
    "\n",
    "train_data = pd.DataFrame(min_max_scaler.fit_transform(train_data))\n",
    "\n",
    "test_data = pd.read_csv('Q4/Test_Data.csv', header= None)\n",
    "test_labels = pd.read_csv('Q4/Test_Labels.csv', header= None)[0]\n",
    "\n",
    "test_data = pd.DataFrame(min_max_scaler.fit_transform(test_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# پیاده‌سازی تابع زیر"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{aligned}\n",
    "     \\hat f(x)  & = argmax_g  Pr(g | X = x) \n",
    "     \\\\\n",
    "     & = argmax_g  \\frac{Pr(x | g) p(g)}{p(x)} \n",
    "     \\\\\n",
    "     & = argmax_g  Pr(x | g) p(g)\n",
    " \\end{aligned}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  بدون در نظر گرفتن دانش اولیه"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{aligned}\n",
    "     \\hat f(x)  & =   argmax_g  Pr(x | g) \n",
    " \\end{aligned}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 664,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_Bayes_class(X,mu_list,cov_list): \n",
    "\n",
    "    scores_list = []\n",
    "    classes = len(mu_list)\n",
    "    \n",
    "    for p in range(classes):\n",
    "        score = multivariate_normal.pdf(X, mean=mu_list[p], cov=cov_list[p])\n",
    "        scores_list.append(score)\n",
    "             \n",
    "    return np.argmax(scores_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 665,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "mu_list = []\n",
    "cov_list = []\n",
    "prior_list = []\n",
    "\n",
    "for i in sorted(train_labels.unique()):\n",
    "    data = train_data[train_labels == i]\n",
    "    mu = data.mean()\n",
    "    cov = data.cov()\n",
    "    prior = np.mean(train_labels == i)\n",
    "    \n",
    "    mu_list.append(mu)\n",
    "    cov_list.append(cov)\n",
    "    prior_list.append(prior)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 666,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = test_data.apply(lambda x: predict_Bayes_class(x, mu_list, cov_list), axis= 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 667,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[258,   0,   2,  37,   1,   0,  26,   0,   5,   0],\n",
       "       [  2, 285,   1,  50,   3,   0,   5,   0,   0,   0],\n",
       "       [  8,   0, 248,   6,  61,   0,  35,   0,  10,   0],\n",
       "       [ 16,   4,   3, 324,  11,   0,  13,   0,   4,   0],\n",
       "       [  2,   0,  21,  24, 286,   0,  24,   0,   8,   0],\n",
       "       [  2,   0,   0,   0,   0, 344,   0,   0,   1,   3],\n",
       "       [ 68,   0,  22,  17, 104,   0,  97,   0,  14,   0],\n",
       "       [  0,   0,   0,   0,   0, 298,   0,   0,   0,  33],\n",
       "       [  4,   0,   1,   7,   0,   1,   6,   0, 350,   0],\n",
       "       [  1,   0,   0,   0,   0,  24,   0,   0,   1, 319]], dtype=int64)"
      ]
     },
     "execution_count": 667,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(test_labels, predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 668,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7174285714285714\n"
     ]
    }
   ],
   "source": [
    "print(np.mean(test_labels == predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  با در نظر گرفتن دانش اولیه"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{aligned}\n",
    "\\hat f(x)  & = argmax_g  Pr(g | X = x) p(g)\n",
    " \\end{aligned}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 669,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_Bayes_class_prior_knowledge(X,mu_list,cov_list, prior_list): \n",
    "\n",
    "    scores_list = []\n",
    "    classes = len(mu_list)\n",
    "    \n",
    "    for p in range(classes):\n",
    "        score = multivariate_normal.pdf(X, mean=mu_list[p], cov=cov_list[p]) * prior_list[p]\n",
    "        scores_list.append(score)\n",
    "             \n",
    "    return np.argmax(scores_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 670,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_prior = test_data.apply(lambda x: predict_Bayes_class_prior_knowledge(x,mu_list,cov_list, prior_list), axis= 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 671,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[258,   0,   2,  37,   1,   0,  26,   0,   5,   0],\n",
       "       [  2, 285,   1,  50,   3,   0,   5,   0,   0,   0],\n",
       "       [  8,   0, 248,   6,  61,   0,  35,   0,  10,   0],\n",
       "       [ 16,   4,   3, 324,  11,   0,  13,   0,   4,   0],\n",
       "       [  2,   0,  21,  24, 286,   0,  24,   0,   8,   0],\n",
       "       [  2,   0,   0,   0,   0, 344,   0,   0,   1,   3],\n",
       "       [ 68,   0,  22,  17, 104,   0,  97,   0,  14,   0],\n",
       "       [  0,   0,   0,   0,   0, 298,   0,   0,   0,  33],\n",
       "       [  4,   0,   1,   7,   0,   1,   6,   0, 350,   0],\n",
       "       [  1,   0,   0,   0,   0,  24,   0,   0,   1, 319]], dtype=int64)"
      ]
     },
     "execution_count": 671,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(test_labels, predicted_prior)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 672,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7174285714285714\n"
     ]
    }
   ],
   "source": [
    "print(np.mean(test_labels == predicted_prior))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " از آنجایی که عملا توزیع لیبل‌ها در داده ما شبیه به هم هستند. اضافه کردن فرض پیشین تاثیری بر دقت نخواهد داشت."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import operator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('Q5/income_Q5.csv')\n",
    "train, test = train_test_split(df, test_size=0.2, random_state = 20)\n",
    "train = train.reset_index(drop = True)\n",
    "test = test.reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ P(evidences \\mid class=A) = P(evidence_1 \\mid class=A) * P(evidence_2 \\mid class=A) * ... * P(evidence_n \\mid class=A) $$\n",
    "\n",
    "$$ P(class=A \\mid evidences) = \\frac{P(evidence_1 \\mid class=A) * P(evidence_2 \\mid class=A) * ... * P(evidence_n \\mid class=A) \\, P(class=A)}{P(evidences)} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NaiveBayes(object):\n",
    "            \n",
    "    def fit(self, X, target):\n",
    "        \n",
    "        self.class_probs = {}\n",
    "        self.class_numbers = {}\n",
    "        labels = X[target].unique()\n",
    "        for l in labels:\n",
    "            self.class_probs[l] = np.mean(train[target] == l)\n",
    "            self.class_numbers[l] = np.sum(train[target] == l)\n",
    "                    \n",
    "        self.features = X.columns.drop(target)\n",
    "            \n",
    "        self.features_probs = {}\n",
    "        \n",
    "        for l in labels:\n",
    "            self.features_probs[l] = {}\n",
    "            for f in self.features:\n",
    "                self.features_probs[l][f] = {}\n",
    "                \n",
    "                \n",
    "        for l in labels:\n",
    "            for f in self.features:\n",
    "                vals = X[f].unique()\n",
    "                for val in vals:\n",
    "                    self.features_probs[l][f][val] = (np.sum(np.logical_and( X[f]==val, X[target]==l)) + 1) / (self.class_numbers[l] + len(vals))\n",
    "    \n",
    "    def predict_a_datapoint(self, X_test):\n",
    "        \n",
    "        log_class_probs ={k: np.log10(v) for k, v in self.class_probs.items()}\n",
    "        \n",
    "        for f in self.features:\n",
    "            value = X_test[f]\n",
    "            for label, v in log_class_probs.items():\n",
    "                log_class_probs[label] += np.log10(self.features_probs[label][f][value])\n",
    "                    \n",
    "        return max(log_class_probs.items(), key=operator.itemgetter(1))[0]\n",
    "    \n",
    "    def predict(self, X_test):\n",
    "        prediction = []\n",
    "        for i in X_test.index:\n",
    "            predict_income = self.predict_a_datapoint(X_test.iloc[i])\n",
    "            prediction.append(predict_income)\n",
    "        return pd.Series(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NaiveBayes()\n",
    "\n",
    "model.fit(train, 'income')\n",
    "\n",
    "prediction = model.predict(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[6159, 1226],\n",
       "       [ 624, 1760]], dtype=int64)"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix , accuracy_score\n",
    "\n",
    "confusion_matrix(test.income, prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8106254478452247\n"
     ]
    }
   ],
   "source": [
    "print(accuracy_score(test.income, prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Laplace Smoothing "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    " \n",
    " \\begin{aligned}\n",
    "      P(w|c)=\\frac{\\text{count}(w,c) +1}{\\text{count}(c) + |V|}\n",
    " \\end{aligned}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "این روش، روش بسیار مهمی است، از این جهت که باعث می‌شود ما بتوانیم تاثیر احتمال‌های صفر یعنی در واقع داده‌هایی که در یک کلاس خاص ندیده‌ایم را از بین ببریم و از آن داده‌هایی که مشاهده نشدند هم استفاده کنیم تا مدل درست‌تر و با دقت‌تری داشته باشیم."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
